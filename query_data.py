from langchain.chains import ConversationalRetrievalChain, ConversationChain, LLMChain
from langchain.vectorstores import Pinecone
from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI
from langchain.chains.question_answering import load_qa_chain
from langchain.prompts import PromptTemplate
from langchain.chains.conversation.memory import ConversationSummaryMemory

# initialized with an OpenAI language model and a key to store chat history
memory = ConversationSummaryMemory(llm=OpenAI(), memory_key="chat_history")

# make the template for question generator component of the chain
CONDENSE_PROMPT = PromptTemplate(
    input_variables=["question"],
    template="""Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.

Follow Up Input: {question}
Standalone question:""")

# make template for document retriever component of the chain
QA_PROMPT = PromptTemplate(
    input_variables=["question", "context"],
    template="""You are an AI assistant providing helpful advice. You are given the following extracted parts of a long document and a question Provide a conversational answer based on the context provided.
If you can't find the answer in the context below, just say "Hmm, I'm not sure." Don't try to make up an answer.

Question: {question}
=========
{context}
=========
Answer in Markdown:""")

# create ConversationalRetrievalChain with Pinecone vectore store for question and answer
def get_chain(
        vectorstore: Pinecone
) -> ConversationalRetrievalChain:

    # determine OpenAI Language model for Q/A generator 
    llm=OpenAI(temperature=0)
    streaming_llm=ChatOpenAI(temperature=0)

    # create chain for generating standalone questions from follow-up inputs, using the conversation history as context
    question_generator = LLMChain(
        llm=llm,
        memory=memory,
        prompt=CONDENSE_PROMPT
    )

    # create chain for retrieving relevant documents from the vectorstore based on the rephrased question generated by question generator
    doc_chain = load_qa_chain(
        streaming_llm,
        chain_type="stuff",
        prompt=QA_PROMPT
    )

    # return ConversationRetrievalChain that answers user questions based on a given document store
    return ConversationalRetrievalChain(
        retriever=vectorstore.as_retriever(),
        combine_docs_chain=doc_chain,
        question_generator=question_generator,
    )